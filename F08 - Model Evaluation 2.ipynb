{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation in Python - Performance measures and class imbalance\n",
    "by María Óskarsdóttir\n",
    "\n",
    "This notebook demonstrates the basics of measuring performance of binary classifiers in Python.  In addition, it shows balancing strategies for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Evaluating performance of binary classifiers.\n",
    "#### We assume that a binary classification model to predict churn has already been built.  Our goal is to measure the perforance of this model using various measures, such as confusion matrix, accuracy and AUC. \n",
    "\n",
    "First, we read in the true classes and predicted probabilties of the target variable on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrueTarget = pd.read_csv('true_value.csv')\n",
    "PredictedProb = pd.read_csv('predictions.csv')\n",
    "print(TrueTarget.Churn.value_counts())\n",
    "PredictedProb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use a cut-off of 0.5 to determine the predicted class of the target variable. This variable is called PredictetTarget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictedTarget=(PredictedProb>0.5)+0\n",
    "print(PredictedTarget.Churn_prob.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the libraries we need for the model evaluation. They come from sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate and inspect the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM=confusion_matrix(TrueTarget,PredictedTarget)\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confusion matrix shows us that 937 non-churners are correctly classified as non-churners and that 204 churners are correctly classified as churners.  \n",
    "We can compute the model accuracy manually as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy=(CM[0,0]+CM[1,1])/CM.sum()\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or using the built in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc=accuracy_score(TrueTarget,PredictedTarget)\n",
    "print('Accuracy: %.3f' % Acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Recall: %.3f'% recall_score(TrueTarget,PredictedTarget))\n",
    "print('Precision: %.3f'% precision_score(TrueTarget,PredictedTarget))\n",
    "print('F1-score: %.3f' % f1_score(TrueTarget,PredictedTarget))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the AUC performance measure, which is cut-off independent. This time we use the predicted churn probabilty vector, `PredictedProb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(TrueTarget,PredictedProb)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally define a function to plot the ROC-curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(TrueTarget,PredictedProb)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:  Balancing data\n",
    "In this part we try some techniques to rebalance an unbalanced data set and investigate the effect it has on the class distribution of the target variable. The function `Counter` lets us see the class distibution.\n",
    "\n",
    "We start by generating a simple synthetic dataset with three variables and a target with two classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=5000, n_features=3, n_informative=3,\n",
    "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.03, 0.97],\n",
    "                           class_sep=0.8, random_state=0)\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use the random over sampling techinique, using different balancing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(sampling_strategy='minority')\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X, y)\n",
    "Counter(y_oversampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0,sampling_strategy=0.2)\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X, y)\n",
    "Counter(y_oversampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the random under sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "X_undersampled, y_undersampled = rus.fit_resample(X, y)\n",
    "Counter(y_undersampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.7)\n",
    "X_undersampled, y_undersampled = rus.fit_resample(X, y)\n",
    "Counter(y_undersampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_smote, y_smote = sm.fit_resample(X, y)\n",
    "Counter(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42,sampling_strategy=0.2)\n",
    "X_smote, y_smote = sm.fit_resample(X, y)\n",
    "Counter(y_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Putting it together. How do balancing techniques affect model performance? \n",
    "\n",
    "We use an imbalanced dataset from the imblearn library to demonstrate that by balancing the dataset, performance can improve.\n",
    "\n",
    "Note: The classification techinique is not important in this example. We use out-of-the-box random forests, but any other binary classifier could be used.\n",
    "\n",
    "We start by fetchin the car_eval_4 dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import fetch_datasets\n",
    "car = fetch_datasets()['car_eval_4']\n",
    "X, y = car.data, car.target\n",
    "y[y==-1]=0\n",
    "Counter(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into train and test sets, using stratified sampling to ensure that the distibution of classes in both sets is the same. We will apply balancing techniques to adjust the class balance of the training set and use the test set to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function `Performance` to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Performance(y_test,X_test):\n",
    "    print('Accuracy: ',accuracy_score(y_test,clf.predict(X_test)))\n",
    "    print('Recall: ',recall_score(y_test,clf.predict(X_test)))\n",
    "    print('Precision: ',precision_score(y_test,clf.predict(X_test),zero_division=0))\n",
    "    print('AUC: ',roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build a model with the data as-is, without rebalancing. The accuracy is very high, but both recall and precision are 0, which indicates that the model is not capturing the monority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "Performance(y_test,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Randomly oversample the minorty class, build another model and evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(sampling_strategy=0.4)\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X_train, y_train)\n",
    "Counter(y_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model has the same accuracy, but the recall and precision have improved. The AUC also improves slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X_oversampled, y_oversampled)\n",
    "Performance(y_test,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
