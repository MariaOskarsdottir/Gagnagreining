{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation in Python - Sampling Strategies\n",
    "by María Óskarsdóttir\n",
    "\n",
    "This notebook demonstrates the basics of splitting a dataset before learning a model in Python.  In addition, it shows balancing strategies for imbalanced data. It uses a churn dataset obtained from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Prepare data\n",
    "We start by preparing the data. We read in the data, encode the variables and seperate the data into variables and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "data = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "# The data can be found at https://www.kaggle.com/blastchar/telco-customer-churn\n",
    "\n",
    "data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dummy variables for catigorical variables with >2 levels\n",
    "dummy_columns = [\"MultipleLines\",\"InternetService\",\"OnlineSecurity\",\n",
    "                 \"OnlineBackup\",\"DeviceProtection\",\"TechSupport\",\n",
    "                 \"StreamingTV\",\"StreamingMovies\",\"Contract\",\n",
    "                 \"PaymentMethod\"]\n",
    "\n",
    "df = pd.get_dummies(data, columns = dummy_columns)\n",
    "\n",
    "#Encode categorical variables with 2 levels\n",
    "enc = LabelEncoder()\n",
    "encode_columns = [\"Churn\",\"PaperlessBilling\",\"PhoneService\",\n",
    "                  \"gender\",\"Partner\",\"Dependents\",\"SeniorCitizen\"]\n",
    "\n",
    "for col in encode_columns:\n",
    "    df[col] = enc.fit_transform(df[col])\n",
    "    \n",
    "#Remove customer ID column\n",
    "del df[\"customerID\"]\n",
    "\n",
    "#Make TotalCharges column numeric, empty strings are zeros\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"],errors = 'coerce').fillna(0)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sepertate the target (y) from the other variables (x).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into x (independent variables) and y (dependent variable, target)\n",
    "y = df[[\"Churn\"]]\n",
    "x = df.drop(\"Churn\", axis=1)\n",
    "print('The size of the data is:', x.shape, 'and of the target vector: ',y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Splitting strategies.\n",
    "Now we can split the data.\n",
    "1. Use splitting. In this case the training data is 80% of the observartions in the original data and the test is 20% of the obervations. This is a random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test and training sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= .2, random_state= 1)\n",
    "print('The size of the training set is:', x_train.shape, 'with the target vector: ', y_train.shape)\n",
    "print('The size of the test set is:', x_test.shape, 'with the target vector: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With 10-fold cross validation. We start by setting up the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold # import KFold\n",
    "kf = KFold(n_splits=10) # Define the split - into 10 folds \n",
    "kf.get_n_splits(x) # returns the number of splitting iterations in the cross-validator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the sizes of the training and test set in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(x):\n",
    "    print('TRAIN:', train_index.shape, 'TEST:', test_index.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Leave one out (LOO) cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut \n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(x)\n",
    "\n",
    "for train_index, test_index in loo.split(x):\n",
    "    print('TRAIN:', train_index.shape, 'TEST:', test_index.shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
